{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a9a14-6873-429a-92ce-3df8a960d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import keras_tuner as kt\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56762b52-c40c-4cfe-9eff-567bc51e4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV files\n",
    "train_df = pd.read_csv('../../Data/train.csv', header=0, names=['class', 'title', 'description'])\n",
    "test_df = pd.read_csv('../../Data/test.csv', header=0, names=['class', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6036a-1c31-4186-ad0c-def673f6ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and description for better context\n",
    "train_df['text'] = train_df['title'] + \" \" + train_df['description']\n",
    "test_df['text'] = test_df['title'] + \" \" + test_df['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eddb5f-40ad-4992-a0e1-0e217d7c047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['class'].to_numpy() - 1  # Convert classes to 0-based index\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['class'].to_numpy() - 1  # Convert classes to 0-based index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c2bcb-9240-4dff-af57-bc00e9e4bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 70338  # Size of the vocabulary\n",
    "embedding_dim_50 = 50\n",
    "embedding_dim_100 = 100  # Dimension of the word embeddings\n",
    "embedding_dim_200 = 200 \n",
    "embedding_dim_300 = 300\n",
    "max_length = 200  # Maximum length of the input sequences\n",
    "num_classes = 4  # AG News has 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447c058-eac3-4c17-ace3-79578b42a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f03d02-be68-4089-840b-adb0c05e0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2b7e0-e531-48e1-a989-6e826015e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fcd80-3f55-448c-83da-1ef578935e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_index_50 = {}\n",
    "with open('../../Data/glove.6B/glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index_50[word] = coefs\n",
    "\n",
    "embeddings_index_100 = {}\n",
    "with open('../../Data/glove.6B/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index_100[word] = coefs\n",
    "\n",
    "embeddings_index_200 = {}\n",
    "with open('../../Data/glove.6B/glove.6B.200d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index_200[word] = coefs\n",
    "\n",
    "embeddings_index_300 = {}\n",
    "with open('../../Data/glove.6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index_300[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5baa50-0942-4c9b-a718-80ad4a896dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "matched = 0\n",
    "notmatched =0\n",
    "not_checked =0\n",
    "embedding_matrix_50 = np.zeros((vocab_size, embedding_dim_50))\n",
    "embedding_matrix_100 = np.zeros((vocab_size, embedding_dim_100))\n",
    "embedding_matrix_200 = np.zeros((vocab_size, embedding_dim_200))\n",
    "embedding_matrix_300 = np.zeros((vocab_size, embedding_dim_300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index_50.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            matched = matched+1\n",
    "            embedding_matrix_50[i] = embedding_vector\n",
    "        else :\n",
    "             notmatched = notmatched+1\n",
    "             embedding_matrix_50[i] = np.zeros(embedding_dim_50)                     \n",
    "    else :\n",
    "        not_checked = not_checked+1\n",
    "        \n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index_100.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_100[i] = embedding_vector\n",
    "        else :\n",
    "             embedding_matrix_100[i] = np.zeros(embedding_dim_100)   \n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index_200.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_200[i] = embedding_vector\n",
    "        else :\n",
    "             embedding_matrix_200[i] = np.zeros(embedding_dim_200)  \n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index_300.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_300[i] = embedding_vector\n",
    "        else :\n",
    "             embedding_matrix_300[i] = np.zeros(embedding_dim_300)   \n",
    "\n",
    "print(\"matched : \", matched)\n",
    "print(\"notmatched : \",notmatched)\n",
    "print(\"not checked : \", not_checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf554e6-a150-49f0-962d-c7d69f4c33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model with GloVe embeddings\n",
    "model_50 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim_50, input_length=max_length,\n",
    "              weights=[embedding_matrix_50], trainable=False),  # Use pre-trained GloVe embeddings\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "model_100 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim_100, input_length=max_length,\n",
    "              weights=[embedding_matrix_100], trainable=False),  # Use pre-trained GloVe embeddings\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "model_200 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim_200, input_length=max_length,\n",
    "              weights=[embedding_matrix_200], trainable=False),  # Use pre-trained GloVe embeddings\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "model_300 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim_300, input_length=max_length,\n",
    "              weights=[embedding_matrix_300], trainable=False),  # Use pre-trained GloVe embeddings\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2ea81-6b80-4f5c-9722-4b99434d10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model explicitly\n",
    "model_50.build(input_shape=(None, max_length))\n",
    "model_100.build(input_shape=(None, max_length))\n",
    "model_200.build(input_shape=(None, max_length))\n",
    "model_300.build(input_shape=(None, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f800b-78b9-4a14-b0fe-091afdf3e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model\n",
    "model_50.summary()\n",
    "model_100.summary()\n",
    "model_200.summary()\n",
    "model_300.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acdb37-bf57-4b15-adfb-0efdbd6165b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_50.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_100.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_200.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_300.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7a567-47e1-4ab8-82ad-55b0c840e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Measure memory before training\n",
    "process = psutil.Process()\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "start_time = time.time()\n",
    "history_50 = model_50.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "memory_used_50 = memory_after - memory_before\n",
    "train_time_50 = end_time - start_time\n",
    "\n",
    "print(\"\\n#######GloVe 50-D#######\")\n",
    "print(\"train time : \",train_time_50,\"Seconds\")\n",
    "print(\"memory_used : \", memory_used_50,\" MB\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "start_time = time.time()\n",
    "history_100 = model_100.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "memory_used_100 = memory_after - memory_before\n",
    "train_time_100 = end_time - start_time\n",
    "\n",
    "print(\"\\n#######GloVe 100-D#######\")\n",
    "print(\"train time : \",train_time_100,\"Seconds\")\n",
    "print(\"memory_used : \", memory_used_100,\" MB\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "start_time = time.time()\n",
    "history_200 = model_200.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "memory_used_200 = memory_after - memory_before\n",
    "train_time_200 = end_time - start_time\n",
    "\n",
    "print(\"\\n#######GloVe 200-D#######\")\n",
    "print(\"tran time : \",train_time_200,\"Seconds\")\n",
    "print(\"memory_used : \", memory_used_200,\" MB\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "memory_before = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "start_time = time.time()\n",
    "history_300 = model_300.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "memory_after = process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "memory_used_300 = memory_after - memory_before\n",
    "train_time_300 = end_time - start_time\n",
    "\n",
    "print(\"\\n#######GloVe 300-D#######\")\n",
    "print(\"tran time : \",train_time_300,\"Seconds\")\n",
    "print(\"memory_used : \", memory_used_300,\" MB\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed2f12-92f4-4912-b8fe-6ebf34250249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.title(\"GloVe 50-D\")\n",
    "plt.plot(history_50.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history_50.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.title(\"GloVe 100-D\")\n",
    "plt.plot(history_100.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history_100.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.title(\"GloVe 200-D\")\n",
    "plt.plot(history_200.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history_200.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.title(\"GloVe 300-D\")\n",
    "plt.plot(history_300.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history_300.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc78103-7fce-4183-a797-1ab739b351e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss_50, accuracy_50 = model_50.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy_50:.4f}\")\n",
    "\n",
    "loss_100, accuracy_100 = model_100.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy_100:.4f}\")\n",
    "\n",
    "loss_200, accuracy_200 = model_200.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy_200:.4f}\")\n",
    "\n",
    "loss_300, accuracy_300 = model_300.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy_300:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf2adb-86e4-40f6-be1c-73ac5f5505bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred_50 = model_50.predict(X_test)\n",
    "y_pred_50 = np.argmax(y_pred_50, axis=1)\n",
    "print(\"\\nClassification Report ( GloVe -50D) :\")\n",
    "print(classification_report(y_test, y_pred_50, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred_100 = model_100.predict(X_test)\n",
    "y_pred_100 = np.argmax(y_pred_100, axis=1)\n",
    "print(\"\\nClassification Report ( GloVe -100D) :\")\n",
    "print(classification_report(y_test, y_pred_100, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred_200 = model_200.predict(X_test)\n",
    "y_pred_200 = np.argmax(y_pred_200, axis=1)\n",
    "print(\"\\nClassification Report ( GloVe -200D) :\")\n",
    "print(classification_report(y_test, y_pred_200, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred_300 = model_300.predict(X_test)\n",
    "y_pred_300 = np.argmax(y_pred_300, axis=1)\n",
    "print(\"\\nClassification Report ( GloVe -300D) :\")\n",
    "print(classification_report(y_test, y_pred_300, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a505cc-d505-4113-9f46-87eff536bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat_50 =  metrics.confusion_matrix(y_test, y_pred_50)\n",
    "individual_class_acc_50   =  confusion_mat_50.diagonal()/confusion_mat_50.sum(axis = 1)\n",
    "print(\"\\nIndividual class Accuracy 50-D : \\n\", individual_class_acc_50 )\n",
    "\n",
    "confusion_mat_100 =  metrics.confusion_matrix(y_test, y_pred_100)\n",
    "individual_class_acc_100   =  confusion_mat_100.diagonal()/confusion_mat_100.sum(axis = 1)\n",
    "print(\"\\nIndividual class Accuracy 100-D : \\n\", individual_class_acc_100 )\n",
    "\n",
    "confusion_mat_200 =  metrics.confusion_matrix(y_test, y_pred_200)\n",
    "individual_class_acc_200   =  confusion_mat_200.diagonal()/confusion_mat_200.sum(axis = 1)\n",
    "print(\"\\nIndividual class Accuracy 200-D : \\n\", individual_class_acc_200 )\n",
    "\n",
    "confusion_mat_300 =  metrics.confusion_matrix(y_test, y_pred_300)\n",
    "individual_class_acc_300   =  confusion_mat_50.diagonal()/confusion_mat_300.sum(axis = 1)\n",
    "print(\"\\nIndividual class Accuracy 300-D : \\n\", individual_class_acc_300 )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879638d-0dd7-417d-8b52-e9120b987efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Class Wise Accuracy 50-D')\n",
    "plt.bar(['World', 'Sports', 'Business', 'Sci/Tech'], individual_class_acc_50)\n",
    "plt.xlabel('News')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Class Wise Accuracy 100-D')\n",
    "plt.bar(['World', 'Sports', 'Business', 'Sci/Tech'], individual_class_acc_100)\n",
    "plt.xlabel('News')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Class Wise Accuracy 200-D')\n",
    "plt.bar(['World', 'Sports', 'Business', 'Sci/Tech'], individual_class_acc_200)\n",
    "plt.xlabel('News')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Class Wise Accuracy 300-D')\n",
    "plt.bar(['World', 'Sports', 'Business', 'Sci/Tech'], individual_class_acc_300)\n",
    "plt.xlabel('News')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd9068-5223-4cb1-bdcb-29d434b46826",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_mat_50,display_labels= ['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix 50-D');\n",
    "plt.show() \n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_mat_100,display_labels= ['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix 100-D');\n",
    "plt.show() \n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_mat_200,display_labels= ['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix 200-D');\n",
    "plt.show() \n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_mat_300,display_labels= ['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix 300-D');\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e486-ec6c-4c81-8418-cd2a97c3b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model 50-D\n",
    "model_50.save('ag_news_cnn_glove_model_50D.h5')\n",
    "print(\"Model 50-D saved!\")\n",
    "\n",
    "# Save the model 100-D\n",
    "model_100.save('ag_news_cnn_glove_model_100D.h5')\n",
    "print(\"Model 100-D saved!\")\n",
    "\n",
    "# Save the model 200-D\n",
    "model_200.save('ag_news_cnn_glove_model_200D.h5')\n",
    "print(\"Model 200-D saved!\")\n",
    "\n",
    "# Save the model 300-D\n",
    "model_300.save('ag_news_cnn_glove_model_300D.h5')\n",
    "print(\"Model 300-D saved!\")\n",
    "\n",
    "# Save the tokenizer (for reuse during inference)\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved!\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa57895-3508-4fc8-8209-cb167383f048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
